[
  {
    "objectID": "GNN-CS224W-Notes/Notes/Lesson 3/3.2_Random Walk Approaches for Node Embeddings.html",
    "href": "GNN-CS224W-Notes/Notes/Lesson 3/3.2_Random Walk Approaches for Node Embeddings.html",
    "title": "Random Walk Approaches for Node Embeddings",
    "section": "",
    "text": "Video: Lecture 3.2 – Random Walk Approaches for Node Embeddings\nOfficial Course Slides: Link to Slides\n\n\n\nThis lecture focuses on leveraging random walks to generate meaningful node embeddings, capturing multi-hop structural information efficiently. Key concepts include:\n\nRandom Walks for Embedding:\nRandom walks naturally capture high-order, multi-hop information by exploring paths through the graph. This means that instead of considering all possible node pairs, the approach focuses on those encountered together during short, fixed-length walks, greatly reducing computational cost.\nMaximum Likelihood Objective:\nThe goal is to learn embeddings that maximize the likelihood of nodes co-occurring in the same random walk. In other words, nodes that are frequently visited together during the random walk should have similar embeddings, measurable via dot product or cosine similarity.\nOptimization Process:\n\nRun Random Walks:\nExecute short, fixed random walks over the graph.\nCollect Co-occurrence Data:\nGather the multiset of nodes visited together in these walks.\nOptimize Embeddings:\nAdjust the node embeddings using an objective that maximizes the likelihood of these co-occurrences.\n\nThis optimization initially involves a double summation over all node pairs (which is expensive, O(|V| x |V|); however, it is alleviated by applying negative sampling.\nNegative Sampling for Efficiency:\nInstead of normalizing over all nodes in the network (which is computationally prohibitive), a carefully chosen subset of “negative” nodes is sampled, typically 5–20 nodes per example. These negative samples are drawn according to a distribution proportional to node degree, which increases stability and efficiency.\nStochastic Gradient Descent (SGD):\nThe complex optimization is efficiently solved using SGD, which approximates the gradients on mini-batches of examples. This makes the method scalable and allows for practical training on large graphs.\nEnhancing Expressivity with Node2vec:\nThe lecture also introduces Node2vec, an extension that makes random walks more expressive:\n\nFlexible Neighborhood Definition:\nNode2vec employs a second-order random walk that flexibly balances local (BFS-like) and global (DFS-like) explorations.\nBiasing the Walk:\nA parameter q is introduced to control this trade-off:\n\nLower q biases the walk to venture further away.\nHigher q encourages staying closer to the starting node.\n\n\nThis adaptive mechanism enhances the diversity of the sampled node contexts, while retaining linear time complexity and parallelizability.\n\n\n\n\n\n\nEfficiency Through Sampling:\nBy focusing on co-occurrences in random walks and using negative sampling, the method sidesteps the prohibitive cost of evaluating all node pairs. This clever strategy turns a potentially O(V^2) problem into an efficient, scalable algorithm.\nExpressivity with Bias:\nThe introduction of second-order, biased random walks (as in Node2vec) provides a flexible notion of “neighborhood” that seamlessly balances between local and global exploration. This innovation is key in adapting the method to diverse graph structures.\nOptimization via SGD:\nUsing stochastic gradient descent to approximate the optimization over random walk co-occurrences highlights the practical aspect of turning a complex objective into a tractable learning problem.\nScalability and Practicality:\nOverall, random walk methods (with enhancements like Node2vec) offer a linear time solution that is highly parallelizable, making them a robust choice for real-world, large-scale graphs.\n\n\n\n\n\n\nDeep Dive into Node2vec:\nExplore the details of Node2vec, paying close attention to how the parameters adjust the balance between breadth-first and depth-first search behaviors in the random walks.\nFurther Reading:\nReview key papers on random walk approaches and Node2vec to gain a deeper theoretical and practical understanding of these methods.\n\n\nEnd of Lecture 3.2 Notes"
  },
  {
    "objectID": "GNN-CS224W-Notes/Notes/Lesson 3/3.2_Random Walk Approaches for Node Embeddings.html#discussion-of-key-concepts",
    "href": "GNN-CS224W-Notes/Notes/Lesson 3/3.2_Random Walk Approaches for Node Embeddings.html#discussion-of-key-concepts",
    "title": "Random Walk Approaches for Node Embeddings",
    "section": "",
    "text": "This lecture focuses on leveraging random walks to generate meaningful node embeddings, capturing multi-hop structural information efficiently. Key concepts include:\n\nRandom Walks for Embedding:\nRandom walks naturally capture high-order, multi-hop information by exploring paths through the graph. This means that instead of considering all possible node pairs, the approach focuses on those encountered together during short, fixed-length walks, greatly reducing computational cost.\nMaximum Likelihood Objective:\nThe goal is to learn embeddings that maximize the likelihood of nodes co-occurring in the same random walk. In other words, nodes that are frequently visited together during the random walk should have similar embeddings, measurable via dot product or cosine similarity.\nOptimization Process:\n\nRun Random Walks:\nExecute short, fixed random walks over the graph.\nCollect Co-occurrence Data:\nGather the multiset of nodes visited together in these walks.\nOptimize Embeddings:\nAdjust the node embeddings using an objective that maximizes the likelihood of these co-occurrences.\n\nThis optimization initially involves a double summation over all node pairs (which is expensive, O(|V| x |V|); however, it is alleviated by applying negative sampling.\nNegative Sampling for Efficiency:\nInstead of normalizing over all nodes in the network (which is computationally prohibitive), a carefully chosen subset of “negative” nodes is sampled, typically 5–20 nodes per example. These negative samples are drawn according to a distribution proportional to node degree, which increases stability and efficiency.\nStochastic Gradient Descent (SGD):\nThe complex optimization is efficiently solved using SGD, which approximates the gradients on mini-batches of examples. This makes the method scalable and allows for practical training on large graphs.\nEnhancing Expressivity with Node2vec:\nThe lecture also introduces Node2vec, an extension that makes random walks more expressive:\n\nFlexible Neighborhood Definition:\nNode2vec employs a second-order random walk that flexibly balances local (BFS-like) and global (DFS-like) explorations.\nBiasing the Walk:\nA parameter q is introduced to control this trade-off:\n\nLower q biases the walk to venture further away.\nHigher q encourages staying closer to the starting node.\n\n\nThis adaptive mechanism enhances the diversity of the sampled node contexts, while retaining linear time complexity and parallelizability."
  },
  {
    "objectID": "GNN-CS224W-Notes/Notes/Lesson 3/3.2_Random Walk Approaches for Node Embeddings.html#my-personal-takeaways",
    "href": "GNN-CS224W-Notes/Notes/Lesson 3/3.2_Random Walk Approaches for Node Embeddings.html#my-personal-takeaways",
    "title": "Random Walk Approaches for Node Embeddings",
    "section": "",
    "text": "Efficiency Through Sampling:\nBy focusing on co-occurrences in random walks and using negative sampling, the method sidesteps the prohibitive cost of evaluating all node pairs. This clever strategy turns a potentially O(V^2) problem into an efficient, scalable algorithm.\nExpressivity with Bias:\nThe introduction of second-order, biased random walks (as in Node2vec) provides a flexible notion of “neighborhood” that seamlessly balances between local and global exploration. This innovation is key in adapting the method to diverse graph structures.\nOptimization via SGD:\nUsing stochastic gradient descent to approximate the optimization over random walk co-occurrences highlights the practical aspect of turning a complex objective into a tractable learning problem.\nScalability and Practicality:\nOverall, random walk methods (with enhancements like Node2vec) offer a linear time solution that is highly parallelizable, making them a robust choice for real-world, large-scale graphs."
  },
  {
    "objectID": "GNN-CS224W-Notes/Notes/Lesson 3/3.2_Random Walk Approaches for Node Embeddings.html#next-steps",
    "href": "GNN-CS224W-Notes/Notes/Lesson 3/3.2_Random Walk Approaches for Node Embeddings.html#next-steps",
    "title": "Random Walk Approaches for Node Embeddings",
    "section": "",
    "text": "Deep Dive into Node2vec:\nExplore the details of Node2vec, paying close attention to how the parameters adjust the balance between breadth-first and depth-first search behaviors in the random walks.\nFurther Reading:\nReview key papers on random walk approaches and Node2vec to gain a deeper theoretical and practical understanding of these methods.\n\n\nEnd of Lecture 3.2 Notes"
  },
  {
    "objectID": "GNN-CS224W-Notes/Notes/Lesson 3/3.1_Node Embeddings.html",
    "href": "GNN-CS224W-Notes/Notes/Lesson 3/3.1_Node Embeddings.html",
    "title": "Node Embeddings",
    "section": "",
    "text": "Video: Lecture 3.1 – Node Embeddings\nOfficial Course Slides: Link to Slides\n\n\n\nThis lecture delves into the challenges of manual feature engineering for nodes and introduces representation learning as a solution to automatically learn these features. Key concepts include:\n\nChallenges of Manual Feature Engineering:\nFinding good features for nodes is difficult and often insufficient to capture the complex relationships in large graphs. This limitation sets the stage for representation learning.\nRepresentation Learning for Nodes:\nThe goal is to automatically learn node features such that nodes that are similar in the original graph are also similar in the embedding space.\n\nSimilarity in Embedding Space:\nSimilarity is typically defined using vector operations. The dot product or cosine similarity (the angle between two vectors) is used to quantify how close two node embeddings are. This means that if two nodes are close in the network, their respective vectors should have a high dot product (or a small angular difference), making them “similar” in the learned space.\n\nEncoder and Low-Dimensional Embeddings:\nThe lecture (especially slide 11, which is very important) introduces the concept of an encoder. The encoder maps nodes into low-dimensional vector representations. In its simplest form, this is just an embedding lookup in a large matrix (with one column per node).\n\nScalability Challenge: For very large graphs, the embedding matrix becomes huge (with a size of O(N×d), where N is the number of nodes and d is the embedding dimension).\n\nHow to Define Node Similarity:\nIn this context, node similarity in the embedding space is measured by comparing the vector representations:\n\nDot Product: A higher dot product indicates greater similarity.\nCosine Similarity: By normalizing the vectors, similarity can be captured via the cosine of the angle between them.\nThese measures ensure that nodes with similar connectivity patterns in the original graph are represented by vectors that are close together.\n\nEarly Methods and Future Directions:\nThe lecture briefly introduces early methods like DeepWalk and node2vec that laid the foundation for learning node embeddings.\n\n\n\n\n\n\nAutomating Feature Discovery:\nThe difficulty in manually crafting node features makes the case for representation learning compelling. By automatically learning embeddings, we can capture subtle, complex relationships in the graph without hand-engineering features.\nEfficiency and Scalability Challenges:\nAlthough a simple embedding lookup is conceptually elegant, scaling this approach to very large graphs introduces significant challenges. The O(N×d) storage requirement motivates the development of more efficient embedding techniques.\nTask Independence:\nOne of the most exciting aspects is that these node embeddings are task independent. This means they are not tailored for a specific downstream task, they provide a general-purpose representation of graph nodes that can be leveraged for various applications, such as node classification, link prediction, clustering, and beyond.\n\n\n\n\n\n\nDeep Dive into Advanced Methods:\nUpcoming lectures will explore methods such as DeepWalk, node2vec, and others that build on these foundations to offer more scalable and robust techniques for node embedding.\nFurther Reading:\nReview seminal papers on DeepWalk, node2vec, and other representation learning frameworks to deepen your understanding of the evolution and innovations in node embedding techniques.\n\n\nEnd of Lecture 3.1 Notes"
  },
  {
    "objectID": "GNN-CS224W-Notes/Notes/Lesson 3/3.1_Node Embeddings.html#discussion-of-key-concepts",
    "href": "GNN-CS224W-Notes/Notes/Lesson 3/3.1_Node Embeddings.html#discussion-of-key-concepts",
    "title": "Node Embeddings",
    "section": "",
    "text": "This lecture delves into the challenges of manual feature engineering for nodes and introduces representation learning as a solution to automatically learn these features. Key concepts include:\n\nChallenges of Manual Feature Engineering:\nFinding good features for nodes is difficult and often insufficient to capture the complex relationships in large graphs. This limitation sets the stage for representation learning.\nRepresentation Learning for Nodes:\nThe goal is to automatically learn node features such that nodes that are similar in the original graph are also similar in the embedding space.\n\nSimilarity in Embedding Space:\nSimilarity is typically defined using vector operations. The dot product or cosine similarity (the angle between two vectors) is used to quantify how close two node embeddings are. This means that if two nodes are close in the network, their respective vectors should have a high dot product (or a small angular difference), making them “similar” in the learned space.\n\nEncoder and Low-Dimensional Embeddings:\nThe lecture (especially slide 11, which is very important) introduces the concept of an encoder. The encoder maps nodes into low-dimensional vector representations. In its simplest form, this is just an embedding lookup in a large matrix (with one column per node).\n\nScalability Challenge: For very large graphs, the embedding matrix becomes huge (with a size of O(N×d), where N is the number of nodes and d is the embedding dimension).\n\nHow to Define Node Similarity:\nIn this context, node similarity in the embedding space is measured by comparing the vector representations:\n\nDot Product: A higher dot product indicates greater similarity.\nCosine Similarity: By normalizing the vectors, similarity can be captured via the cosine of the angle between them.\nThese measures ensure that nodes with similar connectivity patterns in the original graph are represented by vectors that are close together.\n\nEarly Methods and Future Directions:\nThe lecture briefly introduces early methods like DeepWalk and node2vec that laid the foundation for learning node embeddings."
  },
  {
    "objectID": "GNN-CS224W-Notes/Notes/Lesson 3/3.1_Node Embeddings.html#my-personal-takeaways",
    "href": "GNN-CS224W-Notes/Notes/Lesson 3/3.1_Node Embeddings.html#my-personal-takeaways",
    "title": "Node Embeddings",
    "section": "",
    "text": "Automating Feature Discovery:\nThe difficulty in manually crafting node features makes the case for representation learning compelling. By automatically learning embeddings, we can capture subtle, complex relationships in the graph without hand-engineering features.\nEfficiency and Scalability Challenges:\nAlthough a simple embedding lookup is conceptually elegant, scaling this approach to very large graphs introduces significant challenges. The O(N×d) storage requirement motivates the development of more efficient embedding techniques.\nTask Independence:\nOne of the most exciting aspects is that these node embeddings are task independent. This means they are not tailored for a specific downstream task, they provide a general-purpose representation of graph nodes that can be leveraged for various applications, such as node classification, link prediction, clustering, and beyond."
  },
  {
    "objectID": "GNN-CS224W-Notes/Notes/Lesson 3/3.1_Node Embeddings.html#next-steps",
    "href": "GNN-CS224W-Notes/Notes/Lesson 3/3.1_Node Embeddings.html#next-steps",
    "title": "Node Embeddings",
    "section": "",
    "text": "Deep Dive into Advanced Methods:\nUpcoming lectures will explore methods such as DeepWalk, node2vec, and others that build on these foundations to offer more scalable and robust techniques for node embedding.\nFurther Reading:\nReview seminal papers on DeepWalk, node2vec, and other representation learning frameworks to deepen your understanding of the evolution and innovations in node embedding techniques.\n\n\nEnd of Lecture 3.1 Notes"
  },
  {
    "objectID": "GNN-CS224W-Notes/Notes/Lesson 1/1.3_Choice of Graph Representation.html",
    "href": "GNN-CS224W-Notes/Notes/Lesson 1/1.3_Choice of Graph Representation.html",
    "title": "Choice of Graph Representation",
    "section": "",
    "text": "Video: Lecture 1.3 – Choice of Graph Representation\nOfficial Course Slides: Stanford CS224W\n\n\n\n\nBuilding a Graph:\n\nDeciding on the object of interest is crucial. In some cases, the choice is unambiguous, while in others, multiple valid options exist—making the decision particularly important.\n\nGraph Types:\n\nUndirected vs. Directed:\n\nRefer to the Glossary for definitions on node degree and other related terms.\n\nBipartite Graphs:\n\nGraphs that consist of two distinct sets of nodes with edges only between sets.\n\n\nGraph Representations:\n\nAdjacency Matrix:\n\nA common method to represent graphs.\nThese matrices are typically sparse (e.g., in human social networks or email graphs).\n\nAdjacency List:\n\nOften more efficient and easier to use for sparse graphs.\n\n\nNodes and Edges Properties:\n\nBoth nodes and edges can carry additional properties such as weight, ranking, type, sign, etc.\n\nMultigraphs:\n\nGraphs that allow more than one edge between the same pair of nodes.\n\nConnectivity Concepts:\n- General Connectivity:\n- A graph is considered connected if every node can be reached from every other node via a path; otherwise, there may be isolated nodes. - In graphs with multiple components, the adjacency matrix often displays a block diagonal structure. - Directed Graph Connectivity:\n- Strong Connectivity: A directed graph is strongly connected if, for every pair of nodes (u, v), there exists a directed path from u to v and from v to u.\n- Weak Connectivity: A directed graph is weakly connected if, when all edges are treated as undirected, the graph becomes connected—even though a directed path between every pair of nodes may not exist.\n\nStrongly Connected Components:\n\nThese are maximal subgraphs in which every node is reachable from every other node via directed paths.\n\n\n\n\n\n\n\n\nChoosing the right graph representation is foundational—it shapes how effectively structural information is captured and utilized.\nDeciding what constitutes the object of interest in graph construction can vary by application and is a non-trivial decision.\nFamiliarity with different representations (adjacency matrices vs. lists) and connectivity concepts is key to harnessing the full power of Graph ML.\n\n\n\n\n\n\n“The way a graph is represented can drastically affect the performance of downstream Graph ML tasks.”\n(Paraphrased from the lecture)\n\n\n\n\n\n\nNext lecture on Node Embeddings will give a deeper understanding of how nodes are embedded based on structural and attribute similarities.\nRefer to the Glossary for detailed definitions of key terms like node degree, connectivity, and graph types.\n\n\nEnd of Lecture 1.3 Notes"
  },
  {
    "objectID": "GNN-CS224W-Notes/Notes/Lesson 1/1.3_Choice of Graph Representation.html#key-topics",
    "href": "GNN-CS224W-Notes/Notes/Lesson 1/1.3_Choice of Graph Representation.html#key-topics",
    "title": "Choice of Graph Representation",
    "section": "",
    "text": "Building a Graph:\n\nDeciding on the object of interest is crucial. In some cases, the choice is unambiguous, while in others, multiple valid options exist—making the decision particularly important.\n\nGraph Types:\n\nUndirected vs. Directed:\n\nRefer to the Glossary for definitions on node degree and other related terms.\n\nBipartite Graphs:\n\nGraphs that consist of two distinct sets of nodes with edges only between sets.\n\n\nGraph Representations:\n\nAdjacency Matrix:\n\nA common method to represent graphs.\nThese matrices are typically sparse (e.g., in human social networks or email graphs).\n\nAdjacency List:\n\nOften more efficient and easier to use for sparse graphs.\n\n\nNodes and Edges Properties:\n\nBoth nodes and edges can carry additional properties such as weight, ranking, type, sign, etc.\n\nMultigraphs:\n\nGraphs that allow more than one edge between the same pair of nodes.\n\nConnectivity Concepts:\n- General Connectivity:\n- A graph is considered connected if every node can be reached from every other node via a path; otherwise, there may be isolated nodes. - In graphs with multiple components, the adjacency matrix often displays a block diagonal structure. - Directed Graph Connectivity:\n- Strong Connectivity: A directed graph is strongly connected if, for every pair of nodes (u, v), there exists a directed path from u to v and from v to u.\n- Weak Connectivity: A directed graph is weakly connected if, when all edges are treated as undirected, the graph becomes connected—even though a directed path between every pair of nodes may not exist.\n\nStrongly Connected Components:\n\nThese are maximal subgraphs in which every node is reachable from every other node via directed paths."
  },
  {
    "objectID": "GNN-CS224W-Notes/Notes/Lesson 1/1.3_Choice of Graph Representation.html#my-personal-takeaways",
    "href": "GNN-CS224W-Notes/Notes/Lesson 1/1.3_Choice of Graph Representation.html#my-personal-takeaways",
    "title": "Choice of Graph Representation",
    "section": "",
    "text": "Choosing the right graph representation is foundational—it shapes how effectively structural information is captured and utilized.\nDeciding what constitutes the object of interest in graph construction can vary by application and is a non-trivial decision.\nFamiliarity with different representations (adjacency matrices vs. lists) and connectivity concepts is key to harnessing the full power of Graph ML."
  },
  {
    "objectID": "GNN-CS224W-Notes/Notes/Lesson 1/1.3_Choice of Graph Representation.html#quotes-or-interesting-points",
    "href": "GNN-CS224W-Notes/Notes/Lesson 1/1.3_Choice of Graph Representation.html#quotes-or-interesting-points",
    "title": "Choice of Graph Representation",
    "section": "",
    "text": "“The way a graph is represented can drastically affect the performance of downstream Graph ML tasks.”\n(Paraphrased from the lecture)"
  },
  {
    "objectID": "GNN-CS224W-Notes/Notes/Lesson 1/1.3_Choice of Graph Representation.html#next-steps",
    "href": "GNN-CS224W-Notes/Notes/Lesson 1/1.3_Choice of Graph Representation.html#next-steps",
    "title": "Choice of Graph Representation",
    "section": "",
    "text": "Next lecture on Node Embeddings will give a deeper understanding of how nodes are embedded based on structural and attribute similarities.\nRefer to the Glossary for detailed definitions of key terms like node degree, connectivity, and graph types.\n\n\nEnd of Lecture 1.3 Notes"
  },
  {
    "objectID": "GNN-CS224W-Notes/Notes/Lesson 2/2.2_Traditional Feature-based Methods.html",
    "href": "GNN-CS224W-Notes/Notes/Lesson 2/2.2_Traditional Feature-based Methods.html",
    "title": "Traditional Feature-based Methods: Link",
    "section": "",
    "text": "Video: Lecture 2.2 - Traditional Feature-based Methods\nOfficial Course Slides: [https://web.stanford.edu/class/cs224w/slides/02-nodeemb.pdf)\n\n\n\nThis lecture explores traditional, handcrafted approaches for link prediction, focusing on how to extract and utilize graph structural features to predict the presence of links between nodes. Key topics include:\n\nLink Prediction Overview:\nThe task is formulated as a binary classification problem where, given a pair of nodes, the goal is to predict whether a link exists. Handcrafted features derived from the graph’s topology are used as predictors.\nLocal Similarity Metrics:\nSeveral metrics are introduced to assess the likelihood of a link:\n\nCommon Neighbors: counts the number of shared neighbors between two nodes.\nJaccard Coefficient: ratio of common neighbors on the union of their neighbors.\nAdamic/Adar Index: weighs shared neighbors inversely by their overall connectivity.\n\nKatz Index:\nMoves beyond immediate neighborhoods by summing over the total number of paths between node pairs. A damping factor exponentially penalizes longer paths, thus incorporating both local and global network information.\nAdjacency Matrix Powers:\nThe lecture explains that the k-th power of the adjacency matrix reveals the number of distinct paths of length k between nodes. Under the proper conditions, the infinite series summing the weighted powers of the adjacency matrix can be expressed in a closed form. This provides an elegant mathematical formulation of link prediction that unifies local metrics (like common neighbors) with contributions from longer-range connections.\n\n\n\n\n\n\nCapturing Global Structure:\nTechniques like the Katz Index and adjacency matrix powers allow for the consideration of paths of varying lengths, integrating both local and global information.\nMathematical Elegance:\nThe closed-form expression derived from the weighted sum of adjacency matrix powers offers a powerful and compact way to summarize connectivity, even if it is often computationally intensive for large graphs.\nScalability Concerns:\nTraditional metrics can be computationally expensive on large graphs, emphasizing the importance of exploring scalable alternatives in modern graph learning.\n\n\n\n\n\n\nFurther Reading:\nExplore research on the Katz Index, matrix power methods, and their closed-form solutions to understand the theoretical foundations behind these techniques.\nPractical Applications:\nConsider how these traditional metrics can serve as baseline features when designing and benchmarking modern link prediction algorithms.\n\n\nEnd of Lecture 2.2 Notes"
  },
  {
    "objectID": "GNN-CS224W-Notes/Notes/Lesson 2/2.2_Traditional Feature-based Methods.html#discussion-of-key-concepts",
    "href": "GNN-CS224W-Notes/Notes/Lesson 2/2.2_Traditional Feature-based Methods.html#discussion-of-key-concepts",
    "title": "Traditional Feature-based Methods: Link",
    "section": "",
    "text": "This lecture explores traditional, handcrafted approaches for link prediction, focusing on how to extract and utilize graph structural features to predict the presence of links between nodes. Key topics include:\n\nLink Prediction Overview:\nThe task is formulated as a binary classification problem where, given a pair of nodes, the goal is to predict whether a link exists. Handcrafted features derived from the graph’s topology are used as predictors.\nLocal Similarity Metrics:\nSeveral metrics are introduced to assess the likelihood of a link:\n\nCommon Neighbors: counts the number of shared neighbors between two nodes.\nJaccard Coefficient: ratio of common neighbors on the union of their neighbors.\nAdamic/Adar Index: weighs shared neighbors inversely by their overall connectivity.\n\nKatz Index:\nMoves beyond immediate neighborhoods by summing over the total number of paths between node pairs. A damping factor exponentially penalizes longer paths, thus incorporating both local and global network information.\nAdjacency Matrix Powers:\nThe lecture explains that the k-th power of the adjacency matrix reveals the number of distinct paths of length k between nodes. Under the proper conditions, the infinite series summing the weighted powers of the adjacency matrix can be expressed in a closed form. This provides an elegant mathematical formulation of link prediction that unifies local metrics (like common neighbors) with contributions from longer-range connections."
  },
  {
    "objectID": "GNN-CS224W-Notes/Notes/Lesson 2/2.2_Traditional Feature-based Methods.html#my-personal-takeaways",
    "href": "GNN-CS224W-Notes/Notes/Lesson 2/2.2_Traditional Feature-based Methods.html#my-personal-takeaways",
    "title": "Traditional Feature-based Methods: Link",
    "section": "",
    "text": "Capturing Global Structure:\nTechniques like the Katz Index and adjacency matrix powers allow for the consideration of paths of varying lengths, integrating both local and global information.\nMathematical Elegance:\nThe closed-form expression derived from the weighted sum of adjacency matrix powers offers a powerful and compact way to summarize connectivity, even if it is often computationally intensive for large graphs.\nScalability Concerns:\nTraditional metrics can be computationally expensive on large graphs, emphasizing the importance of exploring scalable alternatives in modern graph learning."
  },
  {
    "objectID": "GNN-CS224W-Notes/Notes/Lesson 2/2.2_Traditional Feature-based Methods.html#next-steps",
    "href": "GNN-CS224W-Notes/Notes/Lesson 2/2.2_Traditional Feature-based Methods.html#next-steps",
    "title": "Traditional Feature-based Methods: Link",
    "section": "",
    "text": "Further Reading:\nExplore research on the Katz Index, matrix power methods, and their closed-form solutions to understand the theoretical foundations behind these techniques.\nPractical Applications:\nConsider how these traditional metrics can serve as baseline features when designing and benchmarking modern link prediction algorithms.\n\n\nEnd of Lecture 2.2 Notes"
  },
  {
    "objectID": "GNN-CS224W-Notes/Notes/Lesson 2/2.3_Traditional Feature-based Methods_Graph.html",
    "href": "GNN-CS224W-Notes/Notes/Lesson 2/2.3_Traditional Feature-based Methods_Graph.html",
    "title": "Traditional Feature-based Methods: Graph",
    "section": "",
    "text": "Video: Lecture 2.3 – Traditional Feature-based Methods: Graph\nOfficial Course Slides: Link to slides\n\n\n\nThis lecture focuses on traditional feature-based approaches for comparing whole graphs using graph kernels. It covers:\n\nGraph Kernels Overview:\nGraph kernels provide a way to measure the similarity between graphs by comparing feature representations extracted from them. These features often come from counts of specific substructures or patterns.\nGraphlets Kernel:\nGraphlets are small, connected subgraphs that capture local topology.\n\nCounting Challenge: Exact counts of graphlets are NP-hard to compute for large graphs, limiting the practical usability of graphlets-based kernels.\nGraphlets serve as a basis for feature vectors describing the graph but because counting them exactly is computationally prohibitive, alternative approaches are often preferred.\n\nWeisfeiler-Lehman (WL) Graph Kernel:\nThe lecture introduces the WL kernel as a more efficient alternative to traditional graphlet kernels.\n\nColor Refinement:\nEvery node starts with the same color. In successive iterations, each node aggregates the colors of its neighbors and applies a hash function to generate a new color. This process refines the node labels, capturing richer structural information.\nFeature Description via Color Counts:\nAfter a series of iterations, the frequency of each color (how many nodes have a given label) forms a feature vector for the graph.\nKernel Computation:\nThe similarity between two graphs is computed as the dot product of their color count feature vectors.\nEfficiency:\nThis method is efficient linear in the number of edges. The maximum number of distinct colors is bounded by the number of nodes, ensuring the overall approach scales linearly with the size of the graph.\n\n\n\n\n\n\n\nBalancing Complexity:\nWhile traditional graphlet-based methods are conceptually appealing, their computational complexity makes them less practical for large graphs. The WL kernel offers a smart compromise, capturing essential structural information in a computationally efficient manner.\nFoundation for Modern Methods:\nUnderstanding these traditional approaches is crucial, as they lay the groundwork for modern graph representation learning. Many advanced methods build on or draw inspiration from these foundational techniques.\n\n\n\n\n\n\nFurther Reading:\nReview foundational literature on graph kernels and the Weisfeiler-Lehman algorithm to deepen your understanding of how these methods extract and leverage graph structure.\nPractical Implementation:\nExperiment with implementing the WL kernel on sample graphs to observe how color refinement and feature aggregation work in practice.\n\n\nEnd of Lecture 2.3 Notes"
  },
  {
    "objectID": "GNN-CS224W-Notes/Notes/Lesson 2/2.3_Traditional Feature-based Methods_Graph.html#discussion-of-key-concepts",
    "href": "GNN-CS224W-Notes/Notes/Lesson 2/2.3_Traditional Feature-based Methods_Graph.html#discussion-of-key-concepts",
    "title": "Traditional Feature-based Methods: Graph",
    "section": "",
    "text": "This lecture focuses on traditional feature-based approaches for comparing whole graphs using graph kernels. It covers:\n\nGraph Kernels Overview:\nGraph kernels provide a way to measure the similarity between graphs by comparing feature representations extracted from them. These features often come from counts of specific substructures or patterns.\nGraphlets Kernel:\nGraphlets are small, connected subgraphs that capture local topology.\n\nCounting Challenge: Exact counts of graphlets are NP-hard to compute for large graphs, limiting the practical usability of graphlets-based kernels.\nGraphlets serve as a basis for feature vectors describing the graph but because counting them exactly is computationally prohibitive, alternative approaches are often preferred.\n\nWeisfeiler-Lehman (WL) Graph Kernel:\nThe lecture introduces the WL kernel as a more efficient alternative to traditional graphlet kernels.\n\nColor Refinement:\nEvery node starts with the same color. In successive iterations, each node aggregates the colors of its neighbors and applies a hash function to generate a new color. This process refines the node labels, capturing richer structural information.\nFeature Description via Color Counts:\nAfter a series of iterations, the frequency of each color (how many nodes have a given label) forms a feature vector for the graph.\nKernel Computation:\nThe similarity between two graphs is computed as the dot product of their color count feature vectors.\nEfficiency:\nThis method is efficient linear in the number of edges. The maximum number of distinct colors is bounded by the number of nodes, ensuring the overall approach scales linearly with the size of the graph."
  },
  {
    "objectID": "GNN-CS224W-Notes/Notes/Lesson 2/2.3_Traditional Feature-based Methods_Graph.html#my-personal-takeaways",
    "href": "GNN-CS224W-Notes/Notes/Lesson 2/2.3_Traditional Feature-based Methods_Graph.html#my-personal-takeaways",
    "title": "Traditional Feature-based Methods: Graph",
    "section": "",
    "text": "Balancing Complexity:\nWhile traditional graphlet-based methods are conceptually appealing, their computational complexity makes them less practical for large graphs. The WL kernel offers a smart compromise, capturing essential structural information in a computationally efficient manner.\nFoundation for Modern Methods:\nUnderstanding these traditional approaches is crucial, as they lay the groundwork for modern graph representation learning. Many advanced methods build on or draw inspiration from these foundational techniques."
  },
  {
    "objectID": "GNN-CS224W-Notes/Notes/Lesson 2/2.3_Traditional Feature-based Methods_Graph.html#next-steps",
    "href": "GNN-CS224W-Notes/Notes/Lesson 2/2.3_Traditional Feature-based Methods_Graph.html#next-steps",
    "title": "Traditional Feature-based Methods: Graph",
    "section": "",
    "text": "Further Reading:\nReview foundational literature on graph kernels and the Weisfeiler-Lehman algorithm to deepen your understanding of how these methods extract and leverage graph structure.\nPractical Implementation:\nExperiment with implementing the WL kernel on sample graphs to observe how color refinement and feature aggregation work in practice.\n\n\nEnd of Lecture 2.3 Notes"
  },
  {
    "objectID": "GNN-CS224W-Notes/Glossary.html",
    "href": "GNN-CS224W-Notes/Glossary.html",
    "title": "GNN CS224W Notes",
    "section": "",
    "text": "This glossary provides definitions for key terms referenced in these lecture notes on Graph Neural Networks. Each entry is organized alphabetically for quick reference."
  },
  {
    "objectID": "GNN-CS224W-Notes/Glossary.html#a",
    "href": "GNN-CS224W-Notes/Glossary.html#a",
    "title": "GNN CS224W Notes",
    "section": "A",
    "text": "A\n\nAdjacency List:\nA data structure used to represent a graph, where each node is associated with a list of its neighboring nodes. This is particularly efficient for sparse graphs.\nAdjacency Matrix:\nA square matrix used to represent a graph. Each element at row i and column j indicates the presence (and possibly the weight) of an edge between nodes i and j. In sparse graphs, most entries are zero."
  },
  {
    "objectID": "GNN-CS224W-Notes/Glossary.html#b",
    "href": "GNN-CS224W-Notes/Glossary.html#b",
    "title": "GNN CS224W Notes",
    "section": "B",
    "text": "B\n\nBipartite Graph:\nA graph whose vertices can be divided into two disjoint sets such that every edge connects a vertex from one set to a vertex from the other set. There are no edges connecting vertices within the same set."
  },
  {
    "objectID": "GNN-CS224W-Notes/Glossary.html#c",
    "href": "GNN-CS224W-Notes/Glossary.html#c",
    "title": "GNN CS224W Notes",
    "section": "C",
    "text": "C\n\nConnectivity:\nA property of a graph that determines whether there is a path between every pair of nodes. A graph is connected if every node can be reached from every other node.\nConnected Component:\nA subgraph in which any two vertices are connected by paths, and which is not connected to any additional vertices in the overall graph."
  },
  {
    "objectID": "GNN-CS224W-Notes/Glossary.html#d",
    "href": "GNN-CS224W-Notes/Glossary.html#d",
    "title": "GNN CS224W Notes",
    "section": "D",
    "text": "D\n\nDirected Graph (Digraph):\nA graph in which edges have a direction, indicating a one-way relationship between nodes.\nDegree (Node Degree):\nThe number of edges incident to a node. In directed graphs, this can be divided into in-degree (number of incoming edges) and out-degree (number of outgoing edges)."
  },
  {
    "objectID": "GNN-CS224W-Notes/Glossary.html#g",
    "href": "GNN-CS224W-Notes/Glossary.html#g",
    "title": "GNN CS224W Notes",
    "section": "G",
    "text": "G\n\nGraph:\nA mathematical structure composed of a set of nodes (vertices) and a set of edges that connect pairs of nodes. Graphs are used to model relationships between entities.\nGraph Representations:\nMethods for encoding a graph into a data structure that can be processed by algorithms, such as using an adjacency matrix or an adjacency list."
  },
  {
    "objectID": "GNN-CS224W-Notes/Glossary.html#m",
    "href": "GNN-CS224W-Notes/Glossary.html#m",
    "title": "GNN CS224W Notes",
    "section": "M",
    "text": "M\n\nManual Feature Engineering:\nThe process of manually selecting or designing features from raw data for use in machine learning models. In Graph ML, representation learning often reduces the need for extensive manual feature engineering.\nMultigraph:\nA graph that allows multiple edges (parallel edges) between the same pair of nodes."
  },
  {
    "objectID": "GNN-CS224W-Notes/Glossary.html#n",
    "href": "GNN-CS224W-Notes/Glossary.html#n",
    "title": "GNN CS224W Notes",
    "section": "N",
    "text": "N\n\nNode:\nThe fundamental unit or vertex in a graph, representing an individual entity.\nNode Embedding:\nA learned vector representation of a node in a d-dimensional space that captures the node’s properties and its structural position within the graph."
  },
  {
    "objectID": "GNN-CS224W-Notes/Glossary.html#r",
    "href": "GNN-CS224W-Notes/Glossary.html#r",
    "title": "GNN CS224W Notes",
    "section": "R",
    "text": "R\n\nRepresentation Learning:\nThe process by which a model automatically learns to represent data (such as nodes in a graph) in a way that is useful for downstream tasks, minimizing the need for manual feature engineering."
  },
  {
    "objectID": "GNN-CS224W-Notes/Glossary.html#s",
    "href": "GNN-CS224W-Notes/Glossary.html#s",
    "title": "GNN CS224W Notes",
    "section": "S",
    "text": "S\n\nSpatial Graph:\nA graph in which nodes have a physical or geometric interpretation, such as representing amino acids in a protein where edges are defined based on spatial proximity.\nStrong Connectivity:\nIn a directed graph, the condition where for every pair of nodes (u, v), there exists a directed path from u to v and from v to u.\nStrongly Connected Components:\nMaximal subgraphs of a directed graph in which every node is reachable from every other node via directed paths.\nWeak Connectivity:\nIn a directed graph, the graph is weakly connected if replacing all directed edges with undirected edges results in a connected graph."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Course Overview",
    "section": "",
    "text": "Welcome to my personal notes on the Stanford Graph Neural Networks (GNN) Course.\nThese notes capture my own takeaways, clarifications and code examples from each lecture. Since there are many lessons, I keep their detailed list in a separate note."
  },
  {
    "objectID": "index.html#key-sections",
    "href": "index.html#key-sections",
    "title": "Course Overview",
    "section": "Key Sections",
    "text": "Key Sections\n\nLessons_index – Complete list of all lessons, each with summaries and links.\nGlossary – Definitions of common GNN terms and acronyms.\nReferences – Links to papers, official resources and anything useful.\nCode_Examples/ – Folder containing practical notebooks for hands-on GNN experiments."
  },
  {
    "objectID": "index.html#how-to-use-these-notes",
    "href": "index.html#how-to-use-these-notes",
    "title": "Course Overview",
    "section": "How to Use These Notes",
    "text": "How to Use These Notes\n\nPersonal Perspective: These are my own summaries of what I’ve learned; they may not be exhaustive.\n\nFull Lesson List: If you want a detailed breakdown of each lecture, check out Lessons_index.\nHands-On Practice: You can explore the Code_Examples folder for GNN implementations and experiments. (coming soon…)"
  },
  {
    "objectID": "index.html#contributing",
    "href": "index.html#contributing",
    "title": "Course Overview",
    "section": "Contributing",
    "text": "Contributing\nIf you see something off or have a better explanation, feel free to open a pull request or suggest edits.  Thanks for reading, let’s dive into GNNs together. :)"
  },
  {
    "objectID": "GNN-CS224W-Notes/Lessons_index.html",
    "href": "GNN-CS224W-Notes/Lessons_index.html",
    "title": "GNN CS224W Notes",
    "section": "",
    "text": "Below is a high-level index for all 60+ lessons in the Stanford GNN Course.\nFeel free to update these groupings if you prefer a different organization."
  },
  {
    "objectID": "GNN-CS224W-Notes/Lessons_index.html#module-1-introduction-basics",
    "href": "GNN-CS224W-Notes/Lessons_index.html#module-1-introduction-basics",
    "title": "GNN CS224W Notes",
    "section": "Module 1: Introduction & Basics",
    "text": "Module 1: Introduction & Basics\n\n1.1_Why_Graphs\n1.2_Applications & Tasks in Graph ML\n1.3_Choice of Graph Representation"
  },
  {
    "objectID": "GNN-CS224W-Notes/Lessons_index.html#module-2-core-gnn-architectures",
    "href": "GNN-CS224W-Notes/Lessons_index.html#module-2-core-gnn-architectures",
    "title": "GNN CS224W Notes",
    "section": "Module 2: Core GNN Architectures",
    "text": "Module 2: Core GNN Architectures"
  },
  {
    "objectID": "GNN-CS224W-Notes/Notes/Lesson 2/2.1_Traditional Feature-based Methods Node.html#discussion-of-key-concepts",
    "href": "GNN-CS224W-Notes/Notes/Lesson 2/2.1_Traditional Feature-based Methods Node.html#discussion-of-key-concepts",
    "title": "Lecture 2.1: Traditional Feature-based Methods: Node",
    "section": "Discussion of Key Concepts",
    "text": "Discussion of Key Concepts\nThis lecture explores traditional, handcrafted approaches to assess node importance within a graph. It covers:\n\nNode Degree:\nThe simplest metric that counts the number of edges incident to a node. It provides a basic measure of connectivity and hints at a node’s potential influence.\nCentrality Measures:\nThese go beyond simple connectivity to evaluate the role of a node within the overall structure of the network.\n\nEigenvector Centrality:\nMeasures a node’s influence by considering the importance of its neighbors. It is computed from the leading eigenvector of the graph’s adjacency matrix (associated with the largest eigenvalue).\nBetweenness Centrality:\nQuantifies how frequently a node lies on the shortest paths between other nodes, indicating its role as a bridge or bottleneck within the network.\nCloseness Centrality:\nAssesses how close a node is to all other nodes based on the average shortest path lengths, showing how quickly information can spread from that node.\nClustering Centrality:\nReflects the degree to which a node’s neighbors are interconnected, offering insights into the local density and community structure.\n\nGraphlets:\nThese are small, connected subgraphs that capture recurring local patterns in the network. Graphlets serve as additional features to describe the local topology around each node, complementing the traditional centrality metrics.\n\nBy combining these techniques, one can obtain a robust and multi-faceted understanding of node importance, which not only aids in interpreting graph structure but also sets the stage for advanced approaches like Graph Neural Networks."
  },
  {
    "objectID": "GNN-CS224W-Notes/Notes/Lesson 2/2.1_Traditional Feature-based Methods Node.html#my-personal-takeaways",
    "href": "GNN-CS224W-Notes/Notes/Lesson 2/2.1_Traditional Feature-based Methods Node.html#my-personal-takeaways",
    "title": "Lecture 2.1: Traditional Feature-based Methods: Node",
    "section": "My Personal Takeaways",
    "text": "My Personal Takeaways\n\nBaseline Interpretability:\nTraditional metrics such as node degree and centrality measures are straightforward and provide an intuitive starting point for assessing node importance.\nImportance of Feature Normalization:\nA critical point from the slides is the need to normalize centrality measures. Normalization ensures that values are comparable across nodes and minimizes the impact of outliers.\nSimplicity vs. Complexity Trade-off:\nWhile handcrafted features are transparent and easy to compute, they may fail to capture complex interactions in the data, highlighting the limitations that motivate the shift toward learning-based methods like GNNs.\nComplementary Insights:\nNo single metric tells the whole story—combining multiple centrality measures with graphlet features leads to a more comprehensive view of node importance.\nScalability and Robustness:\nAn additional important insight from the slides is the challenge of scaling these methods to large graphs and ensuring robustness to noise, issues that further justify the move to automated representation learning with GNNs."
  },
  {
    "objectID": "GNN-CS224W-Notes/Notes/Lesson 2/2.1_Traditional Feature-based Methods Node.html#next-steps",
    "href": "GNN-CS224W-Notes/Notes/Lesson 2/2.1_Traditional Feature-based Methods Node.html#next-steps",
    "title": "Lecture 2.1: Traditional Feature-based Methods: Node",
    "section": "Next Steps",
    "text": "Next Steps\n\nLecture 2.2:\nThe next lecture will focus on GNN-based approaches for node representation learning, addressing the limitations of traditional feature-based methods.\nFurther Reading:\nExplore foundational papers on centrality measures and graphlet analysis for a deeper understanding of these traditional techniques.\n\n\nEnd of Lecture 2.1 Notes"
  },
  {
    "objectID": "GNN-CS224W-Notes/Notes/Lesson 1/1.2 Applications & Tasks in Graph ML.html",
    "href": "GNN-CS224W-Notes/Notes/Lesson 1/1.2 Applications & Tasks in Graph ML.html",
    "title": "Applications & Tasks in Graph ML",
    "section": "",
    "text": "Video: Lecture 1.2 – Applications of Graph ML\nOfficial Course Slides: Stanford CS224W\n\n\n\n\n\n\nProtein Structure & AlphaFold:\n\nModeling the three-dimensional organization of proteins as graphs.\nSpatial Graph Idea:\n\nNodes: Represent amino acids.\n\nEdges: Connect amino acids based on proximity.\n\nUnderpins methods like AlphaFold.\n\n\n\n\n\n\nEdge Prediction & Recommender Systems:\n\nPredicting the existence or strength of edges between nodes.\nLearning embeddings so that related nodes are closer in the d-dimensional space than unrelated ones.\n\nBipartite Graphs in Image Analysis:\n\nCombining image features with graph structure can outperform using image data alone.\n\nPredicting Drug-Protein Impact:\n\nAnalyzing drug-protein interaction networks to forecast the impact of drug combinations and potential side effects.\n\n\n\n\n\n\nTraffic Prediction:\n\nUsing subgraph-level ML tasks to forecast traffic patterns.\nDeployed in production systems like Google Maps. ### Graph-Level Tasks\n\nDrug Discovery & Molecule Generation:\n\nApplying graph-level ML for drug discovery, including deep learning for antibiotic discovery.\nGraph Generation:\n\nGenerating molecules in a targeted way for novel drug design.\n\n\n\n\n\n\n\n\n\nTask Diversity:\nGraph ML spans from node-level tasks (protein modeling) to graph-level tasks (drug discovery), showcasing its versatility.\nRepresentation Learning is Central:\nLearning embeddings that reflect similarity is key to tasks like edge prediction and recommender systems.\nReal-World Impact:\nApplications such as traffic prediction and drug-protein interaction analysis demonstrate how Graph ML can drive practical innovations.\n\n\n\n\n\n\n“Capturing structure at different levels—from individual nodes to entire graphs—unlocks powerful applications across diverse domains.”\n(Paraphrased from the lecture)\n\n\n\n\n\n\nLecture 1.3:\nNext lecture will cover the choice of graph representation, exploring various methods, their trade-offs and how these representations affect the performance of Graph ML applications.\nFurther Reading:\nReview the notes from Lecture 1.2 and consult the Glossary for key terms related to graph representation.\n\n\nEnd of Lecture 1.2 Notes"
  },
  {
    "objectID": "GNN-CS224W-Notes/Notes/Lesson 1/1.2 Applications & Tasks in Graph ML.html#key-topics",
    "href": "GNN-CS224W-Notes/Notes/Lesson 1/1.2 Applications & Tasks in Graph ML.html#key-topics",
    "title": "Applications & Tasks in Graph ML",
    "section": "",
    "text": "Protein Structure & AlphaFold:\n\nModeling the three-dimensional organization of proteins as graphs.\nSpatial Graph Idea:\n\nNodes: Represent amino acids.\n\nEdges: Connect amino acids based on proximity.\n\nUnderpins methods like AlphaFold.\n\n\n\n\n\n\nEdge Prediction & Recommender Systems:\n\nPredicting the existence or strength of edges between nodes.\nLearning embeddings so that related nodes are closer in the d-dimensional space than unrelated ones.\n\nBipartite Graphs in Image Analysis:\n\nCombining image features with graph structure can outperform using image data alone.\n\nPredicting Drug-Protein Impact:\n\nAnalyzing drug-protein interaction networks to forecast the impact of drug combinations and potential side effects.\n\n\n\n\n\n\nTraffic Prediction:\n\nUsing subgraph-level ML tasks to forecast traffic patterns.\nDeployed in production systems like Google Maps. ### Graph-Level Tasks\n\nDrug Discovery & Molecule Generation:\n\nApplying graph-level ML for drug discovery, including deep learning for antibiotic discovery.\nGraph Generation:\n\nGenerating molecules in a targeted way for novel drug design."
  },
  {
    "objectID": "GNN-CS224W-Notes/Notes/Lesson 1/1.2 Applications & Tasks in Graph ML.html#my-personal-takeaways",
    "href": "GNN-CS224W-Notes/Notes/Lesson 1/1.2 Applications & Tasks in Graph ML.html#my-personal-takeaways",
    "title": "Applications & Tasks in Graph ML",
    "section": "",
    "text": "Task Diversity:\nGraph ML spans from node-level tasks (protein modeling) to graph-level tasks (drug discovery), showcasing its versatility.\nRepresentation Learning is Central:\nLearning embeddings that reflect similarity is key to tasks like edge prediction and recommender systems.\nReal-World Impact:\nApplications such as traffic prediction and drug-protein interaction analysis demonstrate how Graph ML can drive practical innovations."
  },
  {
    "objectID": "GNN-CS224W-Notes/Notes/Lesson 1/1.2 Applications & Tasks in Graph ML.html#quotes-or-interesting-points",
    "href": "GNN-CS224W-Notes/Notes/Lesson 1/1.2 Applications & Tasks in Graph ML.html#quotes-or-interesting-points",
    "title": "Applications & Tasks in Graph ML",
    "section": "",
    "text": "“Capturing structure at different levels—from individual nodes to entire graphs—unlocks powerful applications across diverse domains.”\n(Paraphrased from the lecture)"
  },
  {
    "objectID": "GNN-CS224W-Notes/Notes/Lesson 1/1.2 Applications & Tasks in Graph ML.html#next-steps",
    "href": "GNN-CS224W-Notes/Notes/Lesson 1/1.2 Applications & Tasks in Graph ML.html#next-steps",
    "title": "Applications & Tasks in Graph ML",
    "section": "",
    "text": "Lecture 1.3:\nNext lecture will cover the choice of graph representation, exploring various methods, their trade-offs and how these representations affect the performance of Graph ML applications.\nFurther Reading:\nReview the notes from Lecture 1.2 and consult the Glossary for key terms related to graph representation.\n\n\nEnd of Lecture 1.2 Notes"
  },
  {
    "objectID": "GNN-CS224W-Notes/Notes/Lesson 1/1.1_Why_Graphs.html",
    "href": "GNN-CS224W-Notes/Notes/Lesson 1/1.1_Why_Graphs.html",
    "title": "Introduction to Graph Neural Networks",
    "section": "",
    "text": "Video: Lecture 1 – Introduction to GNNs\nOfficial Course Slides: Stanford CS224W\n\n\n\n\nWhy Graphs Matter\n\nMany real-world systems (social networks, molecules, knowledge graphs, etc.) can be modeled as graphs.\nA graph perspective captures the relationships between entities (nodes) through interactions (edges).\n\nTraditional ML vs. Graph ML\n\nTraditional ML often relies on manual feature engineering, treating each sample independently.\nIn Graph ML, the model automatically extracts structural information through representation learning.\n\nHigh-Level Overview of GNNs\n\nGNNs learn node representations by iteratively aggregating and transforming features from neighboring nodes.\nMessage Passing: Nodes receive “messages” from adjacent nodes to update their embeddings.\nMapping Nodes to Embeddings: Each node is assigned a d-dimensional vector (in ℝ^d) so that similar nodes (in terms of structure and attributes) are embedded close together.\n\nUnique Challenges in Graph Data\n\nNo strict spatial locality: Unlike images, graphs don’t have a regular grid structure.\nNo universal reference point: There’s no fixed origin to anchor node positions.\nNo fixed ordering: Nodes are not arranged in a specific order (e.g., left-to-right, top-to-bottom).\n\nCore GNN Tasks\n\nNode-level: Predict labels or properties for individual nodes (e.g., node classification).\nLink-level: Predict the existence or strength of an edge (e.g., friend recommendations).\nGraph-level: Classify or regress properties for entire graphs (e.g., molecule toxicity).\n\nApplications\n\nSocial Networks: Friend recommendations, community detection.\nRecommendation Systems: Product or content suggestions based on user–item interactions.\nSoccer Analytics: GNNs can model complex interactions in sports (e.g., player positions and passing patterns); see Tactics AI for an example. (My work on this coming soon…)\n\n\n\n\n\n\n\nUnified Perspective:\nGNNs bring together various graph-related tasks—node classification, link prediction, and more—under a single framework.\nComplexity of Graph Data:\nGraphs are inherently sparse and irregular, making the design and training of models more challenging compared to regular data formats like images or text.\nRepresentation Learning Advantage:\nWith GNNs, there’s no need for extensive manual feature engineering; the model learns effective embeddings automatically through message passing.\nExcitement About Sports Analytics:\nThe potential to analyze passing networks and player positions using GNNs could revolutionize team tactics and coaching strategies.\n\n\n\n\n\n\n“Graphs are a universal language to describe relationships.”\n(Paraphrased from the lecture)\n\nThis highlights the versatility and broad applicability of graphs across different domains.\n\n\n\n\n\nLesson 1.2:\nIn the next lesson, we’ll explore in depth the Graph ML applications examining node, edge, subgraph and graph-level tasks and their real-world impacts.\nFurther Reading:\nRefer to the Glossary for definitions and the References for foundational papers.\n\n\nEnd of Lesson 01 Notes"
  },
  {
    "objectID": "GNN-CS224W-Notes/Notes/Lesson 1/1.1_Why_Graphs.html#key-topics",
    "href": "GNN-CS224W-Notes/Notes/Lesson 1/1.1_Why_Graphs.html#key-topics",
    "title": "Introduction to Graph Neural Networks",
    "section": "",
    "text": "Why Graphs Matter\n\nMany real-world systems (social networks, molecules, knowledge graphs, etc.) can be modeled as graphs.\nA graph perspective captures the relationships between entities (nodes) through interactions (edges).\n\nTraditional ML vs. Graph ML\n\nTraditional ML often relies on manual feature engineering, treating each sample independently.\nIn Graph ML, the model automatically extracts structural information through representation learning.\n\nHigh-Level Overview of GNNs\n\nGNNs learn node representations by iteratively aggregating and transforming features from neighboring nodes.\nMessage Passing: Nodes receive “messages” from adjacent nodes to update their embeddings.\nMapping Nodes to Embeddings: Each node is assigned a d-dimensional vector (in ℝ^d) so that similar nodes (in terms of structure and attributes) are embedded close together.\n\nUnique Challenges in Graph Data\n\nNo strict spatial locality: Unlike images, graphs don’t have a regular grid structure.\nNo universal reference point: There’s no fixed origin to anchor node positions.\nNo fixed ordering: Nodes are not arranged in a specific order (e.g., left-to-right, top-to-bottom).\n\nCore GNN Tasks\n\nNode-level: Predict labels or properties for individual nodes (e.g., node classification).\nLink-level: Predict the existence or strength of an edge (e.g., friend recommendations).\nGraph-level: Classify or regress properties for entire graphs (e.g., molecule toxicity).\n\nApplications\n\nSocial Networks: Friend recommendations, community detection.\nRecommendation Systems: Product or content suggestions based on user–item interactions.\nSoccer Analytics: GNNs can model complex interactions in sports (e.g., player positions and passing patterns); see Tactics AI for an example. (My work on this coming soon…)"
  },
  {
    "objectID": "GNN-CS224W-Notes/Notes/Lesson 1/1.1_Why_Graphs.html#my-personal-takeaways",
    "href": "GNN-CS224W-Notes/Notes/Lesson 1/1.1_Why_Graphs.html#my-personal-takeaways",
    "title": "Introduction to Graph Neural Networks",
    "section": "",
    "text": "Unified Perspective:\nGNNs bring together various graph-related tasks—node classification, link prediction, and more—under a single framework.\nComplexity of Graph Data:\nGraphs are inherently sparse and irregular, making the design and training of models more challenging compared to regular data formats like images or text.\nRepresentation Learning Advantage:\nWith GNNs, there’s no need for extensive manual feature engineering; the model learns effective embeddings automatically through message passing.\nExcitement About Sports Analytics:\nThe potential to analyze passing networks and player positions using GNNs could revolutionize team tactics and coaching strategies."
  },
  {
    "objectID": "GNN-CS224W-Notes/Notes/Lesson 1/1.1_Why_Graphs.html#quotes-or-interesting-points",
    "href": "GNN-CS224W-Notes/Notes/Lesson 1/1.1_Why_Graphs.html#quotes-or-interesting-points",
    "title": "Introduction to Graph Neural Networks",
    "section": "",
    "text": "“Graphs are a universal language to describe relationships.”\n(Paraphrased from the lecture)\n\nThis highlights the versatility and broad applicability of graphs across different domains."
  },
  {
    "objectID": "GNN-CS224W-Notes/Notes/Lesson 1/1.1_Why_Graphs.html#next-steps",
    "href": "GNN-CS224W-Notes/Notes/Lesson 1/1.1_Why_Graphs.html#next-steps",
    "title": "Introduction to Graph Neural Networks",
    "section": "",
    "text": "Lesson 1.2:\nIn the next lesson, we’ll explore in depth the Graph ML applications examining node, edge, subgraph and graph-level tasks and their real-world impacts.\nFurther Reading:\nRefer to the Glossary for definitions and the References for foundational papers.\n\n\nEnd of Lesson 01 Notes"
  }
]