**Lecture 5.2 – Relational and Iterative Classification**  
**Video:** [Lecture 5.2 – Relational and Iterative Classification](https://www.youtube.com/watch?v=QUO-HQ44EDc&ab_channel=StanfordOnline)  
**SNAP GitHub:** [SNAP GitHub](https://snap-stanford.github.io/cs224w-notes/machine-learning-with-networks/message-passing-and-node-classification)

---

## Discussion of Key Concepts

- **Relational Classifier Recap**  
    Uses both a node’s own features and the labels of its neighbors to capture local label correlations.
    
- **Iterative Classifier**  
    Extends the relational classifier by updating node labels in multiple rounds, refining predictions with evolving neighbor information.
    
- **Summary Vector**  
    For each node v, compute a **summary vector** $s_v$ that aggregates the current label distribution (or other statistics) of v’s neighbors. This vector encodes “what my neighbors think about my label.”
    
- **Two-Stage Training**
    
    1. **Stage 1:** Train a base classifier $C_1​$ using only each node’s feature vector to predict labels.
        
    2. **Stage 2:** Train an enhanced classifier $C_2​$ using both the node’s feature vector and its summary vector $s_v$​ to predict labels.
        
- **Iterative Procedure**
    
    1. **Initialize:** Apply $C_1$​ to assign initial labels to all nodes.
        
    2. **Compute Summaries:** Build each $s_v$ from current neighbor labels.
        
    3. **Update Labels:** Use $C_2$​ on feature $s_v$ to produce new labels.
        
    4. **Repeat:** Recompute summaries and update labels until labels converge or a maximum number of iterations $n_{\max}$​ is reached (convergence is not guaranteed).
        
- **Convergence Considerations**  
    Iterations may oscillate or fail to converge; practical implementations often set a fixed iteration limit and may apply damping (mixing old and new labels) to stabilize.
    

---

## My Personal Takeaways
    
1. **Design of Summary Vectors Is Crucial**  
    How you aggregate neighbor labels (e.g., average one-hot vectors, weighted counts) strongly influences both convergence behavior and final accuracy.
    
2. **Expressivity vs. Stability Trade-off**  
    More complex iterative schemes can model richer dependencies but introduce risks of non-convergence; simple damping or early stopping often strikes the best practical balance.
    

---

## Next Steps

- **Implement on a Benchmark Graph**  
    Write code for the full iterative classification algorithm on Cora or Citeseer, experimenting with different summary-vector definitions (mean label proportions, max-pooling, etc.).
    
- **Further Reading**
    
    - Neville, J., & Jensen, D. “Iterative Classification in Relational Data” (2003)
    - Lu, Q., & Getoor, L. “Link-based Classification” (2003)