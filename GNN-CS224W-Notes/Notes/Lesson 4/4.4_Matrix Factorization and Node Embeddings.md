**Lecture 4.4 – Matrix Factorization and Node Embeddings**  
**Video:** [Lecture 4.4 - Matrix Factorization and Node Embeddings](https://www.youtube.com/watch?v=r12qJZZVtqc&list=PLoROMvodv4rOP-ImU-O1rYRg2RFxomvFp&index=13&ab_channel=StanfordOnline)  
**Official Course Slides:** [Link to Slides](https://web.stanford.edu/class/cs224w/slides/02-nodeemb.pdf)

---

## Discussion of Key Concepts

- **Embedding via Matrix Factorization:**  
    We represent the graph by its adjacency matrix $A$ and seek a low dimensional embedding matrix $Z \in R^{N \times d}$ such that:
			 $$
				A \approx Z\,Z^T
				$$
    We solve $\min_{Z} \|A - Z Z^T\|_F^2$ to find our embeddings, where is the Frobenius norm. This factorization yields node vectors whose inner products reconstruct original link strengths.
    
- **Scalability & Computation:**  
    Computing the full factorization is and memory‑heavy for large graphs. Techniques include truncated SVD or sampling to approximate the Frobenius‑norm minimization.
    
- **Limitations:**
    
    1. **No Inductive Ability:** **only nodes seen during training have embeddings**; new or unseen nodes cannot be embedded without retraining.
        
    2. **Structural Similarity Blindness:** captures identity of neighbors but fails to detect isomorphic or structurally equivalent roles (e.g., two nodes with similar patterns but different neighbor identities yield different embeddings).
        
    3. **Feature Agnosticism:** uses only adjacency structure; ignores potentially useful node attributes or edge labels.
        
- **Bridging to GNNs:**  
    To incorporate both **structure** and **feature** information and to support inductive learning—Graph Neural Networks (GNNs) and deep graph architectures learn parametric aggregation functions. They generalize embeddings to unseen nodes and capture structural patterns beyond direct neighbor identity.
    

---

## My Personal Takeaways:

1. **Frobenius Norm as Graph Memory:**  
    Minimizing is like preserving every edge weight in a compressed memory bank, each dimension in stores a global snapshot of connectivity.
    
2. **Induction Gap as Achilles’ Heel:**  
    Matrix factorization is brilliant at compression but deaf to change: once the graph grows, the old embeddings sit static, requiring full retraining to learn new patterns.
    
3. **From Static Factorization to Dynamic Filtering:**  
    GNNs extend pure MF by learning filters that convolve node features over structure; this shift from memorizing links to learning propagation rules bridges the gap between representation and reasoning on graphs.
    

---

## Next Steps

- **Implement MF Embeddings:**  
    Apply truncated SVD or gradient‑descent on for a small citation or social graph. Visualize how embedding quality (reconstruction error) varies with.
    
- **Further Reading:**
    - Belkin & Niyogi (2001): _Laplacian Eigenmaps for Dimensionality Reduction and Data Representation_        
    - Kipf & Welling (2017): _Semi‑Supervised Classification with Graph Convolutional Networks_
---

**End of Lecture 4.4 Notes**