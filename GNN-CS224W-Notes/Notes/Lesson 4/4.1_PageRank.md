**Lecture 4.1 – PageRank**  
**Video:** [Lecture 4.1 – PageRank](https://www.youtube.com/watch?v=TU0ankRcHmo&ab_channel=StanfordOnline)  
**Official Course Slides:** [Link to Slides](https://web.stanford.edu/class/cs224w/slides/04-PageRank.pdf)

---

## Discussion of Key Concepts

- **Motivation for PageRank:**  
    Web graphs are enormous and noisy; simple in‐/out‐degree statistics treat every link equally and miss how importance flows through the network. PageRank defines a page’s importance **recursively**: links from high‐rank pages count more than links from low‐rank pages.
    
- **Random Surfer Model & Stochastic Matrix:**  
    Model the web as a Markov chain. At each step, a "random surfer" either:
    
    1. With probability α (the _teleportation_ or _restart_ probability), jumps to any page uniformly at random.
        
    2. With probability 1–α, follows one of the current page’s outgoing links chosen uniformly.
        
    
    We encode these transition probabilities in a **stochastic matrix** M, where each column sums to 1. This ensures the total probability is conserved as the surfer moves.
    
- **Stationary Distribution & Fixed‐Point Equation:**  
    The PageRank vector r gives the long‐run fraction of time the surfer spends on each page. It satisfies the **fixed‐point** equation:
    
    > r = (1–α)·M·r + α·(1/N)·1  
    > subject to ∑ᵢ rᵢ = 1
    
	In plain terms, r is the **stationary distribution** of the random‐surfer process—it no longer changes once convergence is reached.
    
- **Google Matrix & Power Iteration:**  
    Combine link‐following and teleportation into the **Google matrix** computing r by iterating:
    > r^(t+1) = G·r^(t)
	Starting from any initial probability vector (e.g., uniform), this power‐method converges rapidly to the unique PageRank vector.
    
- **Handling Dangling Nodes:**  
    Pages with no outgoing links would absorb probability mass. We treat them as if they link to every page uniformly, replacing their column in M with 1/N mass to preserve stochasticity.
    
- **Personalized & Topic‐Sensitive PageRank:**  
    To bias the ranks toward a set of seed pages, replace the uniform teleport vector (1/N·1) with a custom distribution v over those seeds. The same iteration yields ranks that reflect topic or user preferences.
    

---

## My Personal Takeaways

1. **Global vs. Local Importance:**  
    PageRank transforms local link counts into a _global_ centrality measure, prestige propagates through the entire graph rather than stopping at immediate neighbors.
    
2. **Teleportation as Regularization:**  
    The damping factor α not only ensures convergence and eliminates rank sinks, but also provides a simple handle for personalization by adjusting the teleport distribution.
    
3. **Scalable & Sparse Computation:**  
    Each iteration is just a sparse matrix–vector multiply plus a teleport step. This makes PageRank practical at web scale using distributed systems.
    

---

## Next Steps

- **Implement & Experiment:**  
    Code the power‐method iteration for r = G·r on a small web graph. Try different α values (e.g., 0.15 vs. 0.05) and plot the convergence ‖r^(t+1)–r^(t)‖ over iterations.
    
- **Read Foundational Papers:**  
    Review "The PageRank Citation Ranking: Bringing Order to the Web" (Page et al., 1999) and explore follow‐up work on accelerated solvers and personalized PageRank algorithms.
    

---

**End of Lecture 4.1 Notes**