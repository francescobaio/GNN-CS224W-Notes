# Lecture 6.2 – Basics of Deep Learning  

**Video:** [Lecture 6.2 – Basics of Deep Learning](https://www.youtube.com/watch?v=9zGEOiGkVjU&ab_channel=StanfordOnline)  
**Official Course Slides:** [Link to Slides](https://web.stanford.edu/class/cs224w/slides/06-gnn-intro.pdf)  
**SNAP GitHub:** [SNAP GitHub](https://github.com/snap-stanford/snap)  

---

## Discussion of Key Concepts

- **ML as Optimization**  
  Formulate learning as  
  \[
    \min_{\Theta} \; \mathcal{L}(y, f_{\Theta}(x))
  \]  
  where \(\Theta\) is the set of parameters, \(x\) the input, \(y\) the true label, and \(\mathcal{L}\) the loss.

- **Loss Functions**  
  - **L2 Loss:** \(\|y - f(x)\|_2^2\)  
  - **Cross-Entropy (CE):** for one-hot \(y\) and softmax \(f(x)\):  
    \[
      \mathrm{CE}(y, f(x)) = -\sum_{i=1}^C y_i \log f(x)_i
    \]  
  - **Total Training Loss:**  
    \[
      \mathcal{L}_{\mathrm{total}} = \sum_{(x,y)\in\mathcal{T}} \mathcal{L}(y, f_{\Theta}(x))
    \]

- **Gradient-Based Optimization**  
  - Compute gradient \(\nabla_{\Theta}\mathcal{L}\) to find downhill direction.  
  - **Gradient Descent:** \(\Theta \leftarrow \Theta - \eta \nabla_{\Theta}\mathcal{L}\), where \(\eta\) is the learning rate.  
  - **Stopping Criteria:**  
    - \(\|\nabla \mathcal{L}\|\approx 0\)  
    - No validation‐set improvement  

- **Stochastic Gradient Descent (SGD)**  
  - Approximate full gradient using a **mini-batch** of size \(b\).  
  - **Iteration:** one update on a mini-batch  
  - **Epoch:** one full pass over the dataset  
  - **Properties & Challenges:**  
    1. Unbiased estimate of true gradient  
    2. No convergence guarantee without proper schedule  
    3. Requires careful tuning of \(\eta\), momentum, etc.

- **Backpropagation**  
  - Use chain rule to compute \(\nabla_{\Theta}\mathcal{L}\) through multiple layers.  
  - Efficiently reuses intermediate activations to propagate errors backward.

---

## My Personal Takeaways

1. **Optimization Viewpoint:**  
   Seeing training as repeated descent steps clarifies the role of hyperparameters (learning rate, batch size) in convergence.

2. **Mini-Batch Trade-Off:**  
   Small batches yield noisy gradients (fast updates), large batches more stable but slower per update—find the sweet spot.

3. **Backprop as Automation:**  
   Chain-rule-based gradient computation abstracts away the complexity of deep architectures, letting us focus on model design.

---

## Next Steps

- **Implement from Scratch:**  
  Write a two-layer MLP with manual backprop (no autograd) on MNIST to solidify understanding of gradients and updates.

- **Experiment with Optimizers:**  
  Compare SGD, SGD+momentum, Adam on a toy dataset; track training/validation loss curves.

- **Further Reading:**  
  - Goodfellow, Bengio & Courville, “Deep Learning” (2016), Ch. 6–8  
  - Ruder, “An overview of gradient descent optimization algorithms” (2016)  

