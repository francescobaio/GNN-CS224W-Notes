**Lecture 6.1 – Introduction to Graph Neural Networks**
**Video:** [Lecture 6.1 – Graph Neural Networks – Introduction](https://www.youtube.com/watch?v=F3PgltDzllc&ab_channel=StanfordOnline)  
**Official Course Slides:** [Link to Slides](https://web.stanford.edu/class/cs224w/slides/03-GNN1.pdf)  

---

## Discussion of Key Concepts

- **Two Key Components**  
  1. **Encoder** $\mathcal{E}$ : maps each node’s raw representation (one-hot or features) to a low-dimensional embedding space.  
  2. **Decoder** $\mathcal{D}$ : reconstructs relationships (e.g., via dot-product or cosine similarity) from embeddings.  
  3. **Similarity Function**: defines how distances or scores in embedding space correspond to graph relationships (e.g., edge existence, edge weight).

- **Shallow Encoding (Embedding Lookup)**  
  - Pre-assigns a unique embedding vector to each node.  
  - **Limitations:**  
    0. Requires $(O(N \times d))$ parameters (expensive for large $N$).  
    1. No parameter sharing across nodes.  
    2. **Transductive**: cannot embed unseen nodes at test time.  
    3. Ignores node features and graph structure dynamics.

- **Graph Neural Networks (Deep Graph Encoders)**  
  - Stack multiple layers of **non-linear graph convolutions** or message-passing:  
    \[
      $\mathbf{h}_v^{(k+1)} = \sigma\Bigl( W^{(k)} \cdot \text{AGGREGATE}\bigl(\{\mathbf{h}_u^{(k)}: u\in\mathcal{N}(v)\}\bigr) \Bigr)$
    \]  
  - **Parameter sharing** across nodes and layers enables inductive learning.  
  - Can incorporate node features, edge features, and complex topologies.

- **What Can GNNs Do?**  
  - **Embed Nodes** for downstream tasks (node classification, link prediction).  
  - **Embed Subgraphs/Whole Graphs** for graph classification or similarity.  
  - Handle **arbitrary graph sizes**, **heterogeneous features**, and **variable neighborhood structures**.

- **Why Is This Hard?**  
  Traditional ML expects fixed-size, ordered inputs (images, sequences).  
  Graphs are **unordered**, **variable-size**, and **multi-modal**, requiring specialized architectures to aggregate and propagate information.

---

## My Personal Takeaways

1. **Inductive vs. Transductive:**  
   GNNs generalize to new nodes/graphs by sharing parameters, overcoming shallow embedding limitations.

2. **Topology as Architecture:**  
   The graph structure itself defines how information flows designing aggregation functions is as crucial as choosing network depth.
3. 
---

## Next Steps

- **Further Reading:**
    - Kipf & Welling, “Semi-Supervised Classification with Graph Convolutional Networks” (2017)
    - Hamilton, Ying & Leskovec, “Inductive Representation Learning on Large Graphs (GraphSAGE)” (2017)
