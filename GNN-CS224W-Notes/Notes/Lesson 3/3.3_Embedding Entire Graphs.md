
# Embedding Entire Graphs

**Video:** [Lecture 3.3 – Embedding Entire Graphs](https://www.youtube.com/watch?v=eliMLfJeu7A&list=PLoROMvodv4rOP-ImU-O1rYRg2RFxomvFp&index=9)  
**Official Course Slides:** [Link to Slides](https://web.stanford.edu/class/cs224w/slides/02-nodeemb.pdf)

---

## Discussion of Key Concepts

This lecture focuses on methods to embed an entire graph (or a subgraph) into a continuous vector space. Rather than representing individual nodes, the goal here is to capture the holistic structure of the graph. Key topics include:

- **Graph Embedding Objectives:**  
    The main objective is to map a whole graph (or subgraph) into an embedding space such that similar graphs have embeddings that are close together. Main approaches involve:
    
    - **Averaging Node Embeddings:**  
        A baseline method that simply averages the embeddings of nodes already computed within a graph.
        
    - **Virtual Node Approach:**  
        An improved technique introduces a virtual node whose embedding is learned via methods similar to node2vec. This node acts as a proxy for the entire graph, thereby capturing a more robust summary of the structure.
        
- **Anonymous Walks:**  
    Anonymous walks represent random walks by recording the index of the first time each node is visited, effectively ignoring the actual node identities.
    
    - **Invariant Representation:**  
        Since the anonymous walk depends solely on the order of first visits, two walks with the same order yield the same representation an n-dimensional vector where each dimension represents the probability of a particular anonymous walk pattern in the graph.
        
    - **Random Walk Sampling:**  
        A crucial aspect discussed is determining the number of random walks needed to reliably capture the graph’s structure.
        
- **Learning Embeddings for Entire Graphs:**  
    The idea is to learn a vector Zg that describes the graph. This is done by:
    
    - Running T different random walks on the graph.
        
    - Treating the set (or neighborhood) of random walks as the context, much like in DeepWalk for nodes.
        
    - Optimizing the embeddings to maximize the likelihood of the observed co-occurrences of these random walks.  
    
	The final graph embedding can be used for tasks such as graph classification, where the dot product between embeddings is used to measure similarity or as input features to a neural network.
        
- **Hierarchical Aggregation:**  
    One important question is to find methods to hierarchically aggregate node-level and walk-level information to obtain a comprehensive embedding of the entire graph.


---

## My Personal Takeaways
    
1. **Power of Anonymous Walks:**  
    The anonymous walk approach abstracts away node identities and focuses solely on structural patterns. This invariance allows for a generalized representation of graph topology across different graphs.
    
2. **Trade-offs in Random Walk Sampling:**  
    A key challenge is determining the adequate number of random walks to accurately capture the underlying structure. It’s a delicate balance between sampling enough information and maintaining computational efficiency.
    
3. **Task Independence:**  
    Much like node embeddings, these graph-level embeddings are task-independent. They provide a general-purpose representation that can be plugged into various downstream applications—whether for graph classification, similarity assessment, or as inputs to more complex models.    

---

## Next Steps

- **Explore Hierarchical Methods:**  
    Delve into upcoming lectures and readings that address how to hierarchically aggregate information from nodes and substructures to further refine graph embeddings.
    
- **Further Reading:**  
    Investigate seminal papers on graph kernels and graph embedding methods (Graph2Vec) to deepen your understanding of the theory and applications behind these techniques.
    

---

**End of Lecture 3.3 Notes**