# Random Walk Approaches for Node Embeddings

**Video:** [Lecture 3.2 – Random Walk Approaches for Node Embeddings](https://www.youtube.com/watch?v=Xv0wRy66Big&list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn&index=8&ab_channel=StanfordOnline)  
**Official Course Slides:** [Link to Slides](https://web.stanford.edu/class/cs224w/slides/03-GNN1.pdf)

---

## Discussion of Key Concepts

This lecture focuses on leveraging random walks to generate meaningful node embeddings, capturing multi-hop structural information efficiently. Key concepts include:

- **Random Walks for Embedding:**  
    Random walks naturally capture high-order, multi-hop information by exploring paths through the graph. This means that instead of considering all possible node pairs, the approach focuses on those encountered together during short, fixed-length walks, greatly reducing computational cost.
    
- **Maximum Likelihood Objective:**  
    The goal is to learn embeddings that maximize the likelihood of nodes co-occurring in the same random walk. In other words, nodes that are frequently visited together during the random walk should have similar embeddings, measurable via dot product or cosine similarity.
    
- **Optimization Process:**
    
    1. **Run Random Walks:**  
        Execute short, fixed random walks over the graph.
        
    2. **Collect Co-occurrence Data:**  
        Gather the multiset of nodes visited together in these walks.
        
    3. **Optimize Embeddings:**  
        Adjust the node embeddings using an objective that maximizes the likelihood of these co-occurrences.
        
    
    This optimization initially involves a double summation over all node pairs (which is expensive, O(|V| x |V|); however, it is alleviated by applying negative sampling.
    
- **Negative Sampling for Efficiency:**  
    Instead of normalizing over all nodes in the network (which is computationally prohibitive), a carefully chosen subset of "negative" nodes is sampled, typically 5–20 nodes per example. These negative samples are drawn according to a distribution proportional to node degree, which increases stability and efficiency.
    
- **Stochastic Gradient Descent (SGD):**  
    The complex optimization is efficiently solved using **SGD**, which approximates the gradients on mini-batches of examples. This makes the method scalable and allows for practical training on large graphs.
    
- **Enhancing Expressivity with Node2vec:**  
    The lecture also introduces Node2vec, an extension that makes random walks more expressive:
    
    - **Flexible Neighborhood Definition:**  
        **Node2vec** employs a second-order random walk that flexibly balances local (BFS-like) and global (DFS-like) explorations.
        
    - **Biasing the Walk:**  
        A parameter q is introduced to control this trade-off:
        
        - Lower q biases the walk to venture further away.
            
        - Higher q encourages staying closer to the starting node.
            
    
    This adaptive mechanism enhances the diversity of the sampled node contexts, while retaining linear time complexity and parallelizability.
    

---

## My Personal Takeaways

1. **Efficiency Through Sampling:**  
    By focusing on co-occurrences in random walks and using negative sampling, the method sidesteps the prohibitive cost of evaluating all node pairs. This clever strategy turns a potentially O(V^2) problem into an efficient, scalable algorithm.
    
2. **Expressivity with Bias:**  
    The introduction of second-order, biased random walks (as in Node2vec) provides a flexible notion of “neighborhood” that seamlessly balances between local and global exploration. This innovation is key in adapting the method to diverse graph structures.
    
3. **Optimization via SGD:**  
    Using stochastic gradient descent to approximate the optimization over random walk co-occurrences highlights the practical aspect of turning a complex objective into a tractable learning problem.
    
5. **Scalability and Practicality:**  
    Overall, random walk methods (with enhancements like Node2vec) offer a linear time solution that is highly parallelizable, making them a robust choice for real-world, large-scale graphs.
    

---

## Next Steps

- **Deep Dive into Node2vec:**  
    Explore the details of Node2vec, paying close attention to how the parameters adjust the balance between breadth-first and depth-first search behaviors in the random walks.
    
- **Further Reading:**  
    Review key papers on random walk approaches and Node2vec to gain a deeper theoretical and practical understanding of these methods.
    

---

**End of Lecture 3.2 Notes**

